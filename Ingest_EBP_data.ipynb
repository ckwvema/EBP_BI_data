{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect and fetch data from EBP\n",
    "\n",
    "def retry_on_failure(max_retries: int = 3, delay: float = 1.0):\n",
    "    \"\"\"Decorator to retry function calls on failure.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        logger.error(f\"Function {func.__name__} failed after {max_retries} attempts: {e}\")\n",
    "                        raise\n",
    "                    logger.warning(f\"Attempt {attempt + 1} failed for {func.__name__}: {e}. Retrying in {delay}s...\")\n",
    "                    time.sleep(delay)\n",
    "            return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class EBP:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize EBP client with environment variables.\"\"\"\n",
    "        self.username = os.getenv('EBP_USERNAME')\n",
    "        self.password = os.getenv('EBP_PASSWORD')\n",
    "        self.prefix = os.getenv('EBP_API_PREFIX')\n",
    "        self.default_limit = int(os.getenv('DEFAULT_LIMIT', '20000'))\n",
    "        self.auth_header = None\n",
    "        \n",
    "        if not all([self.username, self.password, self.prefix]):\n",
    "            raise ValueError(\"Missing required environment variables: EBP_USERNAME, EBP_PASSWORD, EBP_API_PREFIX\")\n",
    "        \n",
    "        logger.info(\"Initializing EBP client...\")\n",
    "        self.login()\n",
    "\n",
    "    @retry_on_failure(max_retries=3, delay=2.0)\n",
    "    def login(self) -> bool:\n",
    "        \"\"\"Authenticate with EBP API.\"\"\"\n",
    "        url = f\"{self.prefix}/api/admin/v1/session/authenticate\"\n",
    "        credentials = {\"username\": self.username, \"password\": self.password}\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Attempting to authenticate with EBP API...\")\n",
    "            response = requests.post(url, json=credentials, headers=headers, verify=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            token = response.json().get(\"token\")\n",
    "            if token:\n",
    "                self.auth_header = {\n",
    "                    \"Authorization\": f\"Bearer {token}\", \n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                }\n",
    "                logger.info(\"Successfully authenticated with EBP API\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(\"Token not found in authentication response\")\n",
    "                return False\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Authentication failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    @retry_on_failure(max_retries=3, delay=1.0)\n",
    "    def fetch_data(self, endpoint: str, limit: Optional[int] = None) -> Optional[List[Dict]]:\n",
    "        \"\"\"Fetch data from EBP API endpoint.\"\"\"\n",
    "        if not self.auth_header:\n",
    "            logger.error(\"Not authenticated. Please login first.\")\n",
    "            return None\n",
    "            \n",
    "        url = f\"{self.prefix}{endpoint}\"\n",
    "        limit = limit or self.default_limit\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Fetching data from {endpoint} (limit: {limit})\")\n",
    "            response = requests.get(\n",
    "                url, \n",
    "                params={'limit': limit}, \n",
    "                headers=self.auth_header, \n",
    "                verify=True, \n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            elements = data.get('elements', [])\n",
    "            logger.info(f\"Successfully fetched {len(elements)} records from {endpoint}\")\n",
    "            return elements\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Failed to fetch data from {endpoint}: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def get_managers(self, contracts_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fetch manager data for all contracts.\"\"\"\n",
    "        if contracts_df.empty:\n",
    "            logger.warning(\"No contracts provided for manager data fetching\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        managers = []\n",
    "        total_contracts = len(contracts_df)\n",
    "        logger.info(f\"Fetching manager data for {total_contracts} contracts...\")\n",
    "        \n",
    "        for idx, row in contracts_df.iterrows():\n",
    "            try:\n",
    "                if idx % 100 == 0:  # Progress logging every 100 contracts\n",
    "                    logger.info(f\"Processing contract {idx + 1}/{total_contracts}\")\n",
    "                \n",
    "                response = requests.get(\n",
    "                    f\"{self.prefix}/api/admin/v1/contracts/{row['contractId']}/managers\",\n",
    "                    params={'limit': self.default_limit},\n",
    "                    headers=self.auth_header,\n",
    "                    verify=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    manager_data = response.json()\n",
    "                    if manager_data:  # Check if data exists\n",
    "                        df = pd.DataFrame(manager_data)\n",
    "                        \n",
    "                        # Normalize nested JSON data\n",
    "                        df = pd.concat([\n",
    "                            pd.json_normalize(df['contact']),\n",
    "                            pd.json_normalize(df['address']),\n",
    "                            pd.json_normalize(df['personal']),\n",
    "                            df.drop(columns=['contact', 'address', 'personal'])\n",
    "                        ], axis=1)\n",
    "                        \n",
    "                        # Filter out specific email\n",
    "                        df = df[~df['email'].isin(['zev@ckw.ch'])].reset_index(drop=True)\n",
    "                        \n",
    "                        # Clean and rename columns\n",
    "                        df = df.rename(columns={'id': 'profileId'})\n",
    "                        df = df.drop(columns=['profileId', 'isBlocked', 'party', 'externalUserId', 'externalUserParties', 'activeState'], errors='ignore')\n",
    "                        \n",
    "                        # Add contract information\n",
    "                        df['contractId'] = row['contractId']\n",
    "                        df['contractName'] = row['contractName']\n",
    "                        df['contractActiveState'] = row['contractActiveState']\n",
    "                        df['productName'] = row['productName']\n",
    "                        df['areaName'] = row['areaName']\n",
    "                        \n",
    "                        managers.append(df)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to fetch managers for contract {row['contractId']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not managers:\n",
    "            logger.warning(\"No manager data found\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Combine all manager data\n",
    "        output = pd.concat(managers, ignore_index=True)\n",
    "        \n",
    "        # Select and order columns\n",
    "        expected_columns = [\n",
    "            'contractId', 'contractName', 'contractActiveState', 'productName', 'areaName',\n",
    "            'salutation', 'firstName', 'lastName', 'userType', 'username', 'mobile', \n",
    "            'email', 'telephone', 'street', 'houseNumber', 'postalCode', 'city'\n",
    "        ]\n",
    "        \n",
    "        # Only include columns that exist in the dataframe\n",
    "        available_columns = [col for col in expected_columns if col in output.columns]\n",
    "        output = output[available_columns]\n",
    "        \n",
    "        logger.info(f\"Successfully processed {len(output)} manager records\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions To Transform The Received Data\n",
    "\n",
    "def safe_rename_columns(df: pd.DataFrame, column_mapping: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Safely rename columns, only renaming those that exist.\"\"\"\n",
    "    existing_columns = {old: new for old, new in column_mapping.items() if old in df.columns}\n",
    "    if existing_columns:\n",
    "        df = df.rename(columns=existing_columns)\n",
    "        logger.debug(f\"Renamed columns: {existing_columns}\")\n",
    "    return df\n",
    "\n",
    "def safe_drop_columns(df: pd.DataFrame, columns_to_drop: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Safely drop columns, only dropping those that exist.\"\"\"\n",
    "    existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "    if existing_columns:\n",
    "        df = df.drop(columns=existing_columns)\n",
    "        logger.debug(f\"Dropped columns: {existing_columns}\")\n",
    "    return df\n",
    "\n",
    "def transform_buildings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform buildings data.\"\"\"\n",
    "    logger.info(f\"Transforming buildings data: {len(df)} records\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty buildings dataframe provided\")\n",
    "        return df\n",
    "    \n",
    "    df = safe_rename_columns(df, {\n",
    "        \"id\": \"buildingId\",\n",
    "        \"name\": \"buildingName\",\n",
    "        \"activeState\": \"buildingActiveState\"\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Buildings transformation completed: {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "def transform_utility_units(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform utility units data.\"\"\"\n",
    "    logger.info(f\"Transforming utility units data: {len(df)} records\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty utility units dataframe provided\")\n",
    "        return df\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df = safe_drop_columns(df, [\"participations\", \"participationObjects\"])\n",
    "    \n",
    "    # Rename columns\n",
    "    df = safe_rename_columns(df, {\n",
    "        \"id\": \"utilityUnitId\",\n",
    "        \"name\": \"utilityUnitName\",\n",
    "        \"usageType\": \"utilityUnitUsageType\",\n",
    "        \"activeState\": \"utilityUnitActiveState\"\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Utility units transformation completed: {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "def transform_meters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform meters data.\"\"\"\n",
    "    logger.info(f\"Transforming meters data: {len(df)} records\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty meters dataframe provided\")\n",
    "        return df\n",
    "    \n",
    "    # Rename columns\n",
    "    df = safe_rename_columns(df, {\n",
    "        \"id\": \"meterId\",\n",
    "        \"activeState\": \"meterActiveState\"\n",
    "    })\n",
    "    \n",
    "    # Handle date columns\n",
    "    df['billableTo'] = df['billableTo'].fillna('2099-12-31')\n",
    "    \n",
    "    # Convert date columns with error handling\n",
    "    for date_col in ['billableFrom', 'billableTo']:\n",
    "        if date_col in df.columns:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], format=\"%Y-%m-%d\", errors='coerce')\n",
    "            invalid_dates = df[date_col].isna().sum()\n",
    "            if invalid_dates > 0:\n",
    "                logger.warning(f\"Found {invalid_dates} invalid dates in {date_col}\")\n",
    "    \n",
    "    logger.info(f\"Meters transformation completed: {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "def transform_contracts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform contracts data.\"\"\"\n",
    "    logger.info(f\"Transforming contracts data: {len(df)} records\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty contracts dataframe provided\")\n",
    "        return df\n",
    "    \n",
    "    # Extract product name safely\n",
    "    if 'product' in df.columns:\n",
    "        df['productName'] = df['product'].apply(\n",
    "            lambda x: x.get('name') if isinstance(x, dict) else None\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Product column not found in contracts data\")\n",
    "        df['productName'] = None\n",
    "    \n",
    "    # Rename columns\n",
    "    df = safe_rename_columns(df, {\n",
    "        \"id\": \"contractId\",\n",
    "        \"name\": \"contractName\",\n",
    "        \"activeState\": \"contractActiveState\",\n",
    "    })\n",
    "    \n",
    "    # Select required columns\n",
    "    required_columns = [\n",
    "        'contractId', 'contractName', 'contractActiveState', 'startDate', 'endDate', \n",
    "        'productId', 'productName', 'areaId', 'areaName', 'loadDate'\n",
    "    ]\n",
    "    \n",
    "    # Only include columns that exist\n",
    "    available_columns = [col for col in required_columns if col in df.columns]\n",
    "    df = df[available_columns]\n",
    "    \n",
    "    # Handle date columns\n",
    "    df['endDate'] = df['endDate'].fillna('2099-12-31')\n",
    "    \n",
    "    for date_col in ['startDate', 'endDate']:\n",
    "        if date_col in df.columns:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], format=\"%Y-%m-%d\", errors='coerce')\n",
    "            invalid_dates = df[date_col].isna().sum()\n",
    "            if invalid_dates > 0:\n",
    "                logger.warning(f\"Found {invalid_dates} invalid dates in {date_col}\")\n",
    "    \n",
    "    logger.info(f\"Contracts transformation completed: {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "def transform_areas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform areas data.\"\"\"\n",
    "    logger.info(f\"Transforming areas data: {len(df)} records\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty areas dataframe provided\")\n",
    "        return df\n",
    "    \n",
    "    df = safe_rename_columns(df, {\n",
    "        \"id\": \"areaId\",\n",
    "        \"name\": \"areaName\",\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Areas transformation completed: {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "def transform_profiles(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transform profiles data.\"\"\"\n",
    "    logger.info(f\"Transforming profiles data: {len(df)} records\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty profiles dataframe provided\")\n",
    "        return df\n",
    "    \n",
    "    df = safe_rename_columns(df, {\n",
    "        \"id\": \"profileId\"\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Profiles transformation completed: {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "def apply_data_filters(df: pd.DataFrame, filter_words: List[str], column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Apply filtering based on filter words to a specific column.\"\"\"\n",
    "    if df.empty or column_name not in df.columns:\n",
    "        logger.warning(f\"Cannot apply filters: empty dataframe or column '{column_name}' not found\")\n",
    "        return df\n",
    "    \n",
    "    original_count = len(df)\n",
    "    pattern = '|'.join(filter_words)\n",
    "    mask = df[column_name].str.contains(pattern, case=False, na=False)\n",
    "    filtered_df = df[~mask].reset_index(drop=True)\n",
    "    filtered_count = len(filtered_df)\n",
    "    removed_count = original_count - filtered_count\n",
    "    \n",
    "    if removed_count > 0:\n",
    "        logger.info(f\"Filtered out {removed_count} records containing filter words from {column_name}\")\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EBP client and fetch data\n",
    "logger.info(\"Starting EBP data ingestion process...\")\n",
    "\n",
    "try:\n",
    "    # Initialize EBP client\n",
    "    ebp = EBP()\n",
    "    \n",
    "    # Define data endpoints and transformations\n",
    "    ebp_data = {\n",
    "        \"contracts\": {\n",
    "            'url': \"/api/admin/v1/contracts\",\n",
    "            'transformation': transform_contracts\n",
    "        },\n",
    "        \"areas\": {\n",
    "            'url': \"/api/admin/v1/areas\",\n",
    "            'transformation': transform_areas\n",
    "        },\n",
    "        \"buildings\": {\n",
    "            'url': \"/api/admin/v1/buildings\",\n",
    "            'transformation': transform_buildings\n",
    "        },\n",
    "        \"utility_units\": {\n",
    "            'url': \"/api/admin/v1/utilityUnits\",\n",
    "            'transformation': transform_utility_units\n",
    "        },\n",
    "        \"meters\": {\n",
    "            'url': \"/api/admin/v1/meters\",\n",
    "            'transformation': transform_meters\n",
    "        },\n",
    "        \"profiles\": {\n",
    "            'url': \"/api/admin/v1/profiles\",\n",
    "            'transformation': transform_profiles\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Fetch and transform data\n",
    "    logger.info(\"Fetching data from EBP API...\")\n",
    "    for table_name, config in ebp_data.items():\n",
    "        try:\n",
    "            logger.info(f\"Processing {table_name}...\")\n",
    "            data = ebp.fetch_data(config['url'])\n",
    "            \n",
    "            if data is None:\n",
    "                logger.error(f\"Failed to fetch data for {table_name}\")\n",
    "                continue\n",
    "                \n",
    "            df = pd.DataFrame(data)\n",
    "            df['loadDate'] = datetime.today()\n",
    "            \n",
    "            # Apply transformation\n",
    "            transformed_df = config['transformation'](df)\n",
    "            ebp_data[table_name]['data'] = transformed_df\n",
    "            \n",
    "            logger.info(f\"Successfully processed {table_name}: {len(transformed_df)} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {table_name}: {e}\")\n",
    "            ebp_data[table_name]['data'] = pd.DataFrame()\n",
    "\n",
    "    # Extract dataframes\n",
    "    dfa = ebp_data['areas']['data']\n",
    "    dfb = ebp_data['buildings']['data']\n",
    "    dfc = ebp_data['contracts']['data']\n",
    "    dfu = ebp_data['utility_units']['data']\n",
    "    dfm = ebp_data['meters']['data']\n",
    "    dfp = ebp_data['profiles']['data']\n",
    "\n",
    "    # Add areaId to utility units by matching with buildings\n",
    "    if not dfu.empty and not dfb.empty and 'buildingId' in dfu.columns and 'areaId' in dfb.columns:\n",
    "        logger.info(\"Adding areaId to utility units...\")\n",
    "        building_area_map = dfb.set_index('buildingId')['areaId'].to_dict()\n",
    "        dfu['areaId'] = dfu['buildingId'].map(building_area_map)\n",
    "        logger.info(f\"Added areaId to {dfu['areaId'].notna().sum()} utility units\")\n",
    "    else:\n",
    "        logger.warning(\"Cannot add areaId to utility units: missing required columns or empty dataframes\")\n",
    "\n",
    "    # Apply data filtering\n",
    "    filter_words = os.getenv('FILTER_WORDS', 'delete,geloescht,loeschen,lÃ¶sch,ZEV EMD').split(',')\n",
    "    logger.info(f\"Applying data filters with words: {filter_words}\")\n",
    "    \n",
    "    # Filter areas first\n",
    "    dfa = apply_data_filters(dfa, filter_words, 'areaName')\n",
    "    \n",
    "    if not dfa.empty:\n",
    "        # Get area IDs to filter other dataframes\n",
    "        valid_area_ids = set(dfa['areaId'].tolist())\n",
    "        \n",
    "        # Filter other dataframes by areaId\n",
    "        for df_name, df in [('contracts', dfc), ('utility_units', dfu), ('buildings', dfb)]:\n",
    "            if not df.empty and 'areaId' in df.columns:\n",
    "                original_count = len(df)\n",
    "                df = df[df['areaId'].isin(valid_area_ids)].reset_index(drop=True)\n",
    "                filtered_count = len(df)\n",
    "                logger.info(f\"Filtered {df_name}: {original_count} -> {filtered_count} records\")\n",
    "                \n",
    "                # Update the dataframe in ebp_data\n",
    "                if df_name == 'contracts':\n",
    "                    dfc = df\n",
    "                elif df_name == 'utility_units':\n",
    "                    dfu = df\n",
    "                elif df_name == 'buildings':\n",
    "                    dfb = df\n",
    "\n",
    "    # Fetch manager data\n",
    "    logger.info(\"Fetching manager data...\")\n",
    "    dfam = ebp.get_managers(dfc)\n",
    "    dfam['loadDate'] = datetime.today()\n",
    "    \n",
    "    logger.info(\"Data ingestion process completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data ingestion: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV files\n",
    "def create_export_directories(base_path: str, subdirectories: List[str]) -> None:\n",
    "    \"\"\"Create export directories if they don't exist.\"\"\"\n",
    "    for subdir in subdirectories:\n",
    "        dir_path = Path(base_path) / subdir\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        logger.debug(f\"Created/verified directory: {dir_path}\")\n",
    "\n",
    "def export_dataframe_to_csv(df: pd.DataFrame, file_path: Path, table_name: str) -> bool:\n",
    "    \"\"\"Export dataframe to CSV with error handling.\"\"\"\n",
    "    try:\n",
    "        if df.empty:\n",
    "            logger.warning(f\"No data to export for {table_name}\")\n",
    "            return False\n",
    "            \n",
    "        df.to_csv(file_path, index=False)\n",
    "        logger.info(f\"Successfully exported {table_name}: {len(df)} records to {file_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to export {table_name} to {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def export_all_data() -> None:\n",
    "    \"\"\"Export all dataframes to CSV files.\"\"\"\n",
    "    logger.info(\"Starting data export process...\")\n",
    "    \n",
    "    try:\n",
    "        # Get configuration from environment\n",
    "        base_path = os.getenv('EXPORT_BASE_PATH')\n",
    "        subdirectories = os.getenv('EXPORT_SUBDIRECTORIES', 'areas,contracts,buildings,utility_units,meters,managers,profiles').split(',')\n",
    "        \n",
    "        if not base_path:\n",
    "            raise ValueError(\"EXPORT_BASE_PATH not set in environment variables\")\n",
    "        \n",
    "        # Create export directories\n",
    "        create_export_directories(base_path, subdirectories)\n",
    "        \n",
    "        # Generate timestamp for filenames\n",
    "        today = datetime.today().strftime(\"%Y%m%d\")\n",
    "        \n",
    "        # Define data to export\n",
    "        data_to_export = {\n",
    "            'areas': dfa,\n",
    "            'contracts': dfc,\n",
    "            'buildings': dfb,\n",
    "            'utility_units': dfu,\n",
    "            'meters': dfm,\n",
    "            'managers': dfam,\n",
    "            'profiles': dfp\n",
    "        }\n",
    "        \n",
    "        # Export each dataframe\n",
    "        export_results = {}\n",
    "        for table_name, df in data_to_export.items():\n",
    "            if table_name in subdirectories:\n",
    "                file_path = Path(base_path) / table_name / f'{today}_{table_name}.csv'\n",
    "                success = export_dataframe_to_csv(df, file_path, table_name)\n",
    "                export_results[table_name] = success\n",
    "            else:\n",
    "                logger.warning(f\"Subdirectory '{table_name}' not configured for export\")\n",
    "        \n",
    "        # Summary\n",
    "        successful_exports = sum(export_results.values())\n",
    "        total_exports = len(export_results)\n",
    "        logger.info(f\"Export completed: {successful_exports}/{total_exports} files exported successfully\")\n",
    "        \n",
    "        # Log failed exports\n",
    "        failed_exports = [name for name, success in export_results.items() if not success]\n",
    "        if failed_exports:\n",
    "            logger.warning(f\"Failed exports: {failed_exports}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data export: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute export\n",
    "export_all_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
